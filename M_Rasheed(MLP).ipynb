{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzNTVN5B3w0iRFqnl0dj/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gptchat12370-ai/Group-A-DLI-Assignment/blob/main/M_Rasheed(MLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Minimal MLP pipeline (CICIDS2017) — ready to enhance =====\n",
        "import time, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers"
      ],
      "metadata": {
        "id": "69TotNxWdOBO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ config ------------------\n",
        "DATASET_PATH = \"/content/cic_0.01km.csv\"\n",
        "RANDOM_STATE  = 42\n",
        "USE_ADASYN    = True                   # turn on/off imbalance handling\n",
        "BATCH         = 1024                   # try 512/2048 too\n",
        "EPOCHS        = 50                     # raise with early stopping\n",
        "hp = dict(                             # <<< TUNE THESE >>>\n",
        "    hidden1=512, hidden2=256,\n",
        "    dropout1=0.40, dropout2=0.20,\n",
        "    lr=3e-4, l2=1e-6\n",
        ")"
      ],
      "metadata": {
        "id": "iF3DY3aye6cf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ AutoDP (lean) ------------------\n",
        "def auto_encode(df, target='Label'):\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype=='object' and c!=target]\n",
        "    for c in obj_cols:\n",
        "        df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n",
        "    return df\n",
        "\n",
        "def auto_impute(df):\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    return df.fillna(df.median(numeric_only=True))\n",
        "\n",
        "def scale_fit_transform(X_tr, X_val, X_te):\n",
        "    sc = MinMaxScaler()\n",
        "    return sc.fit_transform(X_tr), sc.transform(X_val), sc.transform(X_te), sc"
      ],
      "metadata": {
        "id": "8jgAUR2ce7Pv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ load & preprocess ------------------\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "df = auto_encode(df)\n",
        "df = auto_impute(df)\n",
        "\n",
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label'].astype(int)\n",
        "\n",
        "# 64/16/20 split with stratification\n",
        "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.20, stratify=y_tmp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# optional imbalance handling (fix: remove n_jobs)\n",
        "if USE_ADASYN:\n",
        "    ada = ADASYN(random_state=RANDOM_STATE, sampling_strategy='auto', n_neighbors=5)\n",
        "    X_train, y_train = ada.fit_resample(X_train, y_train)\n",
        "    class_weights = None   # avoid double-compensation when oversampling\n",
        "else:\n",
        "    # class weights only if not oversampling\n",
        "    cls = np.unique(y_train)\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    class_weights = {c: w for c, w in zip(cls, compute_class_weight('balanced', classes=cls, y=y_train))}\n",
        "\n",
        "# feature scaling for MLP\n",
        "X_train, X_val, X_test, scaler = scale_fit_transform(X_train, X_val, X_test)"
      ],
      "metadata": {
        "id": "9CAp3slVe-rn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ model ------------------\n",
        "def make_mlp(input_dim, hp):\n",
        "    reg = regularizers.l2(hp['l2'])\n",
        "    inp = layers.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(hp['hidden1'], activation='relu', kernel_regularizer=reg)(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(hp['dropout1'])(x)\n",
        "    x = layers.Dense(hp['hidden2'], activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(hp['dropout2'])(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=hp['lr']),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = make_mlp(X_train.shape[1], hp)\n",
        "\n",
        "cbs = [\n",
        "    callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=0)\n",
        "]\n",
        "\n",
        "t0 = time.time()\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0,\n",
        "    callbacks=cbs\n",
        ")\n",
        "train_time = time.time() - t0"
      ],
      "metadata": {
        "id": "onEh9Os1fCvB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Boost v3 — Accuracy-first (patched for None class_weights) ====\n",
        "import time, numpy as np, tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "FT_EPOCHS        = 5\n",
        "FT_PATIENCE      = 2\n",
        "POS_W_MULTS      = [0.3, 0.5, 0.7, 1.0]\n",
        "DENSE_SWEEP_STEPS= 401\n",
        "USE_MC_DROPOUT   = False\n",
        "MC_PASSES        = 15\n",
        "\n",
        "def _predict_probs(mdl, X, mc=False, n=15):\n",
        "    if not mc:\n",
        "        return mdl.predict(X, verbose=0).ravel()\n",
        "    outs = []\n",
        "    for _ in range(n):\n",
        "        outs.append(mdl(X, training=True).numpy().ravel())\n",
        "    return np.mean(np.stack(outs, axis=1), axis=1)\n",
        "\n",
        "def _dense_best_threshold(y_true, probs, center=None, steps=401, width=0.2):\n",
        "    if center is None:\n",
        "        ths = np.linspace(0.01, 0.99, 99)\n",
        "        accs = [accuracy_score(y_true, (probs >= t).astype(int)) for t in ths]\n",
        "        center = ths[int(np.argmax(accs))]\n",
        "    lo = max(0.01, center - width/2)\n",
        "    hi = min(0.99, center + width/2)\n",
        "    ths = np.linspace(lo, hi, steps)\n",
        "    accs = [accuracy_score(y_true, (probs >= t).astype(int)) for t in ths]\n",
        "    i = int(np.argmax(accs))\n",
        "    return float(ths[i]), float(accs[i])\n",
        "\n",
        "def _eval_line(name, y, probs, thr):\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    print(f\"{name}: Acc={accuracy_score(y, preds):.5f} | \"\n",
        "          f\"Prec={precision_score(y, preds):.5f} | Rec={recall_score(y, preds):.5f} | \"\n",
        "          f\"F1={f1_score(y, preds):.5f} | ROC-AUC={roc_auc_score(y, probs):.5f} | \"\n",
        "          f\"PR-AUC={average_precision_score(y, probs):.5f} | thr={thr:.3f}\")\n",
        "\n",
        "# ---- A) Base class weights (robust when class_weights is None or missing) ----\n",
        "try:\n",
        "    base_cw = class_weights\n",
        "except NameError:\n",
        "    base_cw = None\n",
        "\n",
        "if not base_cw:\n",
        "    cls = np.unique(y_train)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=cls, y=y_train)\n",
        "    base_cw = {int(c): float(w) for c, w in zip(cls, weights)}\n",
        "    # If perfectly balanced (e.g., after ADASYN), use neutral weights so multipliers matter\n",
        "    if np.isclose(weights.min(), weights.max()):\n",
        "        base_cw = {int(c): 1.0 for c in cls}\n",
        "\n",
        "# Ensure both keys exist\n",
        "base_cw = {0: base_cw.get(0, 1.0), 1: base_cw.get(1, 1.0)}\n",
        "\n",
        "# ---- B) Class-weight fine-tune loop (accuracy-first) ----\n",
        "init_weights = model.get_weights()\n",
        "best_acc, best_state = -1.0, None\n",
        "best_cw_mult = None\n",
        "\n",
        "for m in POS_W_MULTS:\n",
        "    model.set_weights(init_weights)\n",
        "    cw_try = {0: base_cw[0], 1: base_cw[1] * m}  # down-weight positives -> fewer FPs -> higher Acc\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=FT_EPOCHS, batch_size=BATCH,\n",
        "        class_weight=cw_try,\n",
        "        verbose=0,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=FT_PATIENCE, restore_best_weights=True)]\n",
        "    )\n",
        "    val_probs_try = _predict_probs(model, X_val, mc=USE_MC_DROPOUT, n=MC_PASSES)\n",
        "    t_try, acc_try = _dense_best_threshold(y_val, val_probs_try, center=None, steps=121, width=0.6)\n",
        "    if acc_try > best_acc:\n",
        "        best_acc = acc_try\n",
        "        best_cw_mult = m\n",
        "        best_state = (model.get_weights(), t_try)\n",
        "\n",
        "if best_state is not None:\n",
        "    model.set_weights(best_state[0])\n",
        "    center_t = best_state[1]\n",
        "else:\n",
        "    center_t = None\n",
        "\n",
        "# ---- C) Calibration (none vs Platt vs Isotonic) ----\n",
        "val_probs_base  = _predict_probs(model, X_val,  mc=USE_MC_DROPOUT, n=MC_PASSES)\n",
        "test_probs_base = _predict_probs(model, X_test, mc=USE_MC_DROPOUT, n=MC_PASSES)\n",
        "\n",
        "def _platt_fit(probs, y):\n",
        "    p = np.clip(probs, 1e-6, 1-1e-6)\n",
        "    z = np.log(p/(1-p)).reshape(-1,1)\n",
        "    lr = LogisticRegression(max_iter=1000)\n",
        "    lr.fit(z, y)\n",
        "    return lr\n",
        "\n",
        "def _platt_apply(lr, probs):\n",
        "    p = np.clip(probs, 1e-6, 1-1e-6)\n",
        "    z = np.log(p/(1-p)).reshape(-1,1)\n",
        "    return lr.predict_proba(z)[:,1]\n",
        "\n",
        "def _iso_fit(probs, y):\n",
        "    ir = IsotonicRegression(out_of_bounds='clip')\n",
        "    ir.fit(probs, y)\n",
        "    return ir\n",
        "\n",
        "cands = []\n",
        "# none\n",
        "t0, acc0 = _dense_best_threshold(y_val, val_probs_base, center=center_t, steps=DENSE_SWEEP_STEPS, width=0.3)\n",
        "cands.append((\"none\", t0, acc0, val_probs_base, test_probs_base))\n",
        "# platt\n",
        "lr = _platt_fit(val_probs_base, y_val)\n",
        "val_p_pl  = _platt_apply(lr, val_probs_base)\n",
        "test_p_pl = _platt_apply(lr, test_probs_base)\n",
        "t1, acc1 = _dense_best_threshold(y_val, val_p_pl, center=center_t, steps=DENSE_SWEEP_STEPS, width=0.3)\n",
        "cands.append((\"platt\", t1, acc1, val_p_pl, test_p_pl))\n",
        "# isotonic\n",
        "ir = _iso_fit(val_probs_base, y_val)\n",
        "val_p_iso  = ir.predict(val_probs_base)\n",
        "test_p_iso = ir.predict(test_probs_base)\n",
        "t2, acc2 = _dense_best_threshold(y_val, val_p_iso, center=center_t, steps=DENSE_SWEEP_STEPS, width=0.3)\n",
        "cands.append((\"isotonic\", t2, acc2, val_p_iso, test_p_iso))\n",
        "\n",
        "calib_name, best_t, _, val_probs_final, test_probs_final = max(cands, key=lambda x: x[2])\n",
        "print(f\"Selected calibration = {calib_name} | tuned thr = {best_t:.3f} | pos-weight mult = {best_cw_mult}\")\n",
        "\n",
        "# ---- D) Final report ----\n",
        "_eval_line(\"Val   \", y_val,  val_probs_final,  best_t)\n",
        "_eval_line(\"Model \", y_test, test_probs_final, best_t)\n",
        "\n",
        "best_t = float(best_t)  # keep for your summary cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIGA3lqVrEpK",
        "outputId": "001c4743-13ac-4b94-c33d-f28e31e8ed83"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected calibration = none | tuned thr = 0.632 | pos-weight mult = 0.7\n",
            "Val   : Acc=0.95915 | Prec=0.84190 | Rec=0.97896 | F1=0.90527 | ROC-AUC=0.99050 | PR-AUC=0.96779 | thr=0.632\n",
            "Model : Acc=0.95866 | Prec=0.84018 | Rec=0.97872 | F1=0.90418 | ROC-AUC=0.99051 | PR-AUC=0.96453 | thr=0.632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== MODEL vs PAPER (CICIDS2017, AutoDP+AutoFE → MLP) ====\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Use tuned threshold if available; otherwise 0.50\n",
        "try:\n",
        "    t = float(best_t)\n",
        "except NameError:\n",
        "    t = 0.50\n",
        "\n",
        "# Evaluate your model (rename to \"Your Model\" instead of \"Test\")\n",
        "probs = model.predict(X_test, verbose=0).ravel()\n",
        "preds = (probs >= t).astype(int)\n",
        "\n",
        "your = dict(\n",
        "    Accuracy = accuracy_score(y_test, preds)*100,\n",
        "    Precision= precision_score(y_test, preds)*100,\n",
        "    Recall   = recall_score(y_test, preds)*100,\n",
        "    F1       = f1_score(y_test, preds)*100\n",
        ")\n",
        "\n",
        "# Build a readable \"Method\" string from your current settings\n",
        "def _get(name, default=None):\n",
        "    try:\n",
        "        return eval(name)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "hp_local   = _get('hp', {}) or {}\n",
        "use_adasyn = bool(_get('USE_ADASYN', False))\n",
        "use_focal  = bool(_get('USE_FOCAL', False))\n",
        "\n",
        "arch = f\"{hp_local.get('hidden1','?')}-{hp_local.get('hidden2','?')}, do={hp_local.get('dropout1','?')}/{hp_local.get('dropout2','?')}, lr={hp_local.get('lr','?')}\"\n",
        "imb  = \"ADASYN\" if use_adasyn else \"ClassWeights\"\n",
        "loss = \"Focal\" if use_focal else \"BinaryCE\"\n",
        "\n",
        "method_yours = f\"AutoDP+AutoFE + MLP ({arch}; {imb}; thr={t:.3f}; loss={loss})\"\n",
        "\n",
        "# Paper baseline (Table VI: AutoDP & AutoFE → MLP on CICIDS2017)\n",
        "paper_row = {\n",
        "    \"Dataset\": \"CICIDS2017\",\n",
        "    \"Procedure\": \"AutoDP+AutoFE\",\n",
        "    \"Algorithm\": \"MLP\",\n",
        "    \"Method\": \"As reported (paper)\",\n",
        "    \"Accuracy (%)\": 85.968,\n",
        "    \"Precision (%)\": 92.069,\n",
        "    \"Recall (%)\": 26.563,\n",
        "    \"F1 (%)\": 44.831,\n",
        "    \"Training Time (s)\": 16.1  # from the table\n",
        "}\n",
        "\n",
        "# Your model row\n",
        "try:\n",
        "    tt = float(train_time)\n",
        "except Exception:\n",
        "    tt = np.nan\n",
        "\n",
        "your_row = {\n",
        "    \"Dataset\": \"CICIDS2017\",\n",
        "    \"Procedure\": \"AutoDP+AutoFE\",\n",
        "    \"Algorithm\": \"MLP\",\n",
        "    \"Method\": method_yours,\n",
        "    \"Accuracy (%)\": your[\"Accuracy\"],\n",
        "    \"Precision (%)\": your[\"Precision\"],\n",
        "    \"Recall (%)\": your[\"Recall\"],\n",
        "    \"F1 (%)\": your[\"F1\"],\n",
        "    \"Training Time (s)\": tt\n",
        "}\n",
        "\n",
        "summary = pd.DataFrame([paper_row, your_row]).round(3)\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# (Optional) save for your appendix\n",
        "# summary.to_csv(\"model_vs_paper_CICIDS2017_MLP.csv\", index=False)\n",
        "# ==== end ===="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcRa7ZafGA_",
        "outputId": "65397b0b-3215-4a85-bfcc-12e9142e9f57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Dataset     Procedure Algorithm                                                                                 Method  Accuracy (%)  Precision (%)  Recall (%)  F1 (%)  Training Time (s)\n",
            "CICIDS2017 AutoDP+AutoFE       MLP                                                                    As reported (paper)        85.968         92.069      26.563  44.831             16.100\n",
            "CICIDS2017 AutoDP+AutoFE       MLP AutoDP+AutoFE + MLP (512-256, do=0.4/0.2, lr=0.0003; ADASYN; thr=0.632; loss=BinaryCE)        95.866         84.018      97.872  90.418             53.362\n"
          ]
        }
      ]
    }
  ]
}