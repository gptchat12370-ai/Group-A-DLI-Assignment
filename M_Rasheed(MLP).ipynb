{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOmYM9EuayB+g/Kvl5me/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gptchat12370-ai/Group-A-DLI-Assignment/blob/main/M_Rasheed(MLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Minimal MLP pipeline (CICIDS2017) — ready to enhance =====\n",
        "import time, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers"
      ],
      "metadata": {
        "id": "69TotNxWdOBO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ config ------------------\n",
        "DATASET_PATH = \"/content/cic_0.01km.csv\"\n",
        "RANDOM_STATE  = 42\n",
        "USE_ADASYN    = True                   # turn on/off imbalance handling\n",
        "BATCH         = 1024                   # try 512/2048 too\n",
        "EPOCHS        = 50                     # raise with early stopping\n",
        "hp = dict(                             # <<< TUNE THESE >>>\n",
        "    hidden1=512, hidden2=256,\n",
        "    dropout1=0.40, dropout2=0.20,\n",
        "    lr=3e-4, l2=1e-6\n",
        ")"
      ],
      "metadata": {
        "id": "iF3DY3aye6cf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ AutoDP (lean) ------------------\n",
        "def auto_encode(df, target='Label'):\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype=='object' and c!=target]\n",
        "    for c in obj_cols:\n",
        "        df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n",
        "    return df\n",
        "\n",
        "def auto_impute(df):\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    return df.fillna(df.median(numeric_only=True))\n",
        "\n",
        "def scale_fit_transform(X_tr, X_val, X_te):\n",
        "    sc = MinMaxScaler()\n",
        "    return sc.fit_transform(X_tr), sc.transform(X_val), sc.transform(X_te), sc"
      ],
      "metadata": {
        "id": "8jgAUR2ce7Pv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ load & preprocess ------------------\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "df = auto_encode(df)\n",
        "df = auto_impute(df)\n",
        "\n",
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label'].astype(int)\n",
        "\n",
        "# 64/16/20 split with stratification\n",
        "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.20, stratify=y_tmp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# optional imbalance handling (fix: remove n_jobs)\n",
        "if USE_ADASYN:\n",
        "    ada = ADASYN(random_state=RANDOM_STATE, sampling_strategy='auto', n_neighbors=5)\n",
        "    X_train, y_train = ada.fit_resample(X_train, y_train)\n",
        "    class_weights = None   # avoid double-compensation when oversampling\n",
        "else:\n",
        "    # class weights only if not oversampling\n",
        "    cls = np.unique(y_train)\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    class_weights = {c: w for c, w in zip(cls, compute_class_weight('balanced', classes=cls, y=y_train))}\n",
        "\n",
        "# feature scaling for MLP\n",
        "X_train, X_val, X_test, scaler = scale_fit_transform(X_train, X_val, X_test)"
      ],
      "metadata": {
        "id": "9CAp3slVe-rn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ model ------------------\n",
        "def make_mlp(input_dim, hp):\n",
        "    reg = regularizers.l2(hp['l2'])\n",
        "    inp = layers.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(hp['hidden1'], activation='relu', kernel_regularizer=reg)(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(hp['dropout1'])(x)\n",
        "    x = layers.Dense(hp['hidden2'], activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(hp['dropout2'])(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=hp['lr']),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = make_mlp(X_train.shape[1], hp)\n",
        "\n",
        "cbs = [\n",
        "    callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=0)\n",
        "]\n",
        "\n",
        "t0 = time.time()\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0,\n",
        "    callbacks=cbs\n",
        ")\n",
        "train_time = time.time() - t0"
      ],
      "metadata": {
        "id": "onEh9Os1fCvB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== MODEL vs PAPER (CICIDS2017, AutoDP+AutoFE → MLP) ====\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Use tuned threshold if available; otherwise 0.50\n",
        "try:\n",
        "    t = float(best_t)\n",
        "except NameError:\n",
        "    t = 0.50\n",
        "\n",
        "# Evaluate your model (rename to \"Your Model\" instead of \"Test\")\n",
        "probs = model.predict(X_test, verbose=0).ravel()\n",
        "preds = (probs >= t).astype(int)\n",
        "\n",
        "your = dict(\n",
        "    Accuracy = accuracy_score(y_test, preds)*100,\n",
        "    Precision= precision_score(y_test, preds)*100,\n",
        "    Recall   = recall_score(y_test, preds)*100,\n",
        "    F1       = f1_score(y_test, preds)*100\n",
        ")\n",
        "\n",
        "# Build a readable \"Method\" string from your current settings\n",
        "def _get(name, default=None):\n",
        "    try:\n",
        "        return eval(name)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "hp_local   = _get('hp', {}) or {}\n",
        "use_adasyn = bool(_get('USE_ADASYN', False))\n",
        "use_focal  = bool(_get('USE_FOCAL', False))\n",
        "\n",
        "arch = f\"{hp_local.get('hidden1','?')}-{hp_local.get('hidden2','?')}, do={hp_local.get('dropout1','?')}/{hp_local.get('dropout2','?')}, lr={hp_local.get('lr','?')}\"\n",
        "imb  = \"ADASYN\" if use_adasyn else \"ClassWeights\"\n",
        "loss = \"Focal\" if use_focal else \"BinaryCE\"\n",
        "\n",
        "method_yours = f\"AutoDP+AutoFE + MLP ({arch}; {imb}; thr={t:.3f}; loss={loss})\"\n",
        "\n",
        "# Paper baseline (Table VI: AutoDP & AutoFE → MLP on CICIDS2017)\n",
        "paper_row = {\n",
        "    \"Dataset\": \"CICIDS2017\",\n",
        "    \"Procedure\": \"AutoDP+AutoFE\",\n",
        "    \"Algorithm\": \"MLP\",\n",
        "    \"Method\": \"As reported (paper)\",\n",
        "    \"Accuracy (%)\": 85.968,\n",
        "    \"Precision (%)\": 92.069,\n",
        "    \"Recall (%)\": 26.563,\n",
        "    \"F1 (%)\": 44.831,\n",
        "    \"Training Time (s)\": 16.1  # from the table\n",
        "}\n",
        "\n",
        "# Your model row\n",
        "try:\n",
        "    tt = float(train_time)\n",
        "except Exception:\n",
        "    tt = np.nan\n",
        "\n",
        "your_row = {\n",
        "    \"Dataset\": \"CICIDS2017\",\n",
        "    \"Procedure\": \"AutoDP+AutoFE\",\n",
        "    \"Algorithm\": \"MLP\",\n",
        "    \"Method\": method_yours,\n",
        "    \"Accuracy (%)\": your[\"Accuracy\"],\n",
        "    \"Precision (%)\": your[\"Precision\"],\n",
        "    \"Recall (%)\": your[\"Recall\"],\n",
        "    \"F1 (%)\": your[\"F1\"],\n",
        "    \"Training Time (s)\": tt\n",
        "}\n",
        "\n",
        "summary = pd.DataFrame([paper_row, your_row]).round(3)\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# (Optional) save for your appendix\n",
        "# summary.to_csv(\"model_vs_paper_CICIDS2017_MLP.csv\", index=False)\n",
        "# ==== end ===="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcRa7ZafGA_",
        "outputId": "e03f9524-b22b-43ec-e21a-ac81cf821882"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Dataset     Procedure Algorithm                                                                                 Method  Accuracy (%)  Precision (%)  Recall (%)  F1 (%)  Training Time (s)\n",
            "CICIDS2017 AutoDP+AutoFE       MLP                                                                    As reported (paper)        85.968         92.069      26.563  44.831             16.100\n",
            "CICIDS2017 AutoDP+AutoFE       MLP AutoDP+AutoFE + MLP (512-256, do=0.4/0.2, lr=0.0003; ADASYN; thr=0.500; loss=BinaryCE)        91.274         69.887      98.759  81.852             61.719\n"
          ]
        }
      ]
    }
  ]
}