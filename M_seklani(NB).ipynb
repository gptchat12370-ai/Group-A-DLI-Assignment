{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedseklani/Group-A-DLI-Assignment/blob/main/M_seklani(NB).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WRC94g0y6ZZO"
      },
      "outputs": [],
      "source": [
        "# import the packages to be used\n",
        "import os       # to create directories and remove files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# set the random seed to ensure the result is reproducible\n",
        "random.seed(10)\n",
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u5xtYXPmwpF9",
        "outputId": "e585158f-cec2-49ce-bffb-848ff9ad4f9c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Dataset/LUFlow/2020/07'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1669367269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mextract_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mzip_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/LUFlow/2020/07'"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 2. Unzip all csv files\n",
        "# ==========================================\n",
        "import os, zipfile, random, pandas as pd\n",
        "\n",
        "zip_folder = \"Dataset/LUFlow/2020/07\"\n",
        "extract_folder = zip_folder\n",
        "\n",
        "for zip_file in os.listdir(zip_folder):\n",
        "    if zip_file.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(os.path.join(zip_folder, zip_file), 'r') as z:\n",
        "            z.extractall(extract_folder)\n",
        "\n",
        "print(\"✅ All files extracted!\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. Combine all csv files into one\n",
        "# ==========================================\n",
        "def combine_csv_files(dataset_folder: str, reduce_sample_size: bool = False):\n",
        "    combined_path = f\"Dataset/dataset_combined/LUFlow_2020_07.csv\"\n",
        "    os.makedirs(\"Dataset/dataset_combined\", exist_ok=True)\n",
        "\n",
        "    # Remove old file if exists\n",
        "    if os.path.isfile(combined_path):\n",
        "        os.remove(combined_path)\n",
        "\n",
        "    for i, file in enumerate(sorted(os.listdir(dataset_folder))):\n",
        "        if not file.endswith(\".csv\"):\n",
        "            continue\n",
        "        with open(os.path.join(dataset_folder, file), \"r\") as infile, open(combined_path, \"a\") as outfile:\n",
        "            for j, line in enumerate(infile):\n",
        "                if 'Label' in line or 'label' in line:  # header\n",
        "                    if i != 0 or j != 0:\n",
        "                        continue\n",
        "                elif reduce_sample_size and random.randint(1, 10) > 1:\n",
        "                    continue\n",
        "                outfile.write(line.replace(' ï¿½ ', '-'))\n",
        "\n",
        "    print(f\"✅ Combined CSV saved to {combined_path}\")\n",
        "    return combined_path\n",
        "\n",
        "combined_csv = combine_csv_files(\"Dataset/LUFlow/2020/07\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Load into pandas\n",
        "# ==========================================\n",
        "df = pd.read_csv(combined_csv)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQB4S-isHAlo"
      },
      "source": [
        "Step 2. Preliminary analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjQgCOghHBWc"
      },
      "outputs": [],
      "source": [
        "# read the dataset\n",
        "luflow2020 = pd.read_csv('/content/Dataset/dataset_combined/LUFlow_2020_07.csv')\n",
        "luflow2020.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ_31ZVcHXpr"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of rows: {luflow2020.shape[0]}\")\n",
        "print(f\"Number of columns: {luflow2020.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CFyUTtuHba0"
      },
      "outputs": [],
      "source": [
        "print(\"Columns in the dataset:\")\n",
        "luflow2020.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_CUHZfnHe8X"
      },
      "outputs": [],
      "source": [
        "print('Class distribution:')\n",
        "luflow2020['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gDruyW-Hf_6"
      },
      "outputs": [],
      "source": [
        "print('Class distribution (normalized):')\n",
        "luflow2020['label'].value_counts()/luflow2020.shape[0]*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOcg9WUjHnP3"
      },
      "source": [
        "Check for null value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXdBfVvHn49"
      },
      "outputs": [],
      "source": [
        "luflow2020_null_count = luflow2020.isnull().sum()\n",
        "luflow2020_null_count = luflow2020_null_count[luflow2020_null_count > 0]\n",
        "print(f\"Rows contain null value: \\n{luflow2020_null_count}\\n\")\n",
        "\n",
        "luflow2020_null_count = luflow2020_null_count / luflow2020.shape[0] * 100\n",
        "print(f\"Rows contain null value (percentage): \\n{luflow2020_null_count}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwqvNyh0HrfW"
      },
      "source": [
        "Check for infinity value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17lAPf5HsAL"
      },
      "outputs": [],
      "source": [
        "print('Number of samples contains infinity value:')\n",
        "np.isinf(luflow2020.iloc[:, :-2]).any(axis=1).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sG8uXeMHvzw"
      },
      "source": [
        "Check for columns that contain string values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKjq41lyHx_-"
      },
      "outputs": [],
      "source": [
        "luflow2020.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJnEpi9lH0in"
      },
      "source": [
        "Check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KflWy5iH1FF"
      },
      "outputs": [],
      "source": [
        "# check for duplicated column\n",
        "luflow2020.columns[luflow2020.columns.value_counts() > 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfA50MnuH3tM"
      },
      "outputs": [],
      "source": [
        "luflow_duplicates = luflow2020[luflow2020.duplicated()]\n",
        "print(f\"{luflow_duplicates.shape[0]} rows are duplicates\")\n",
        "print(f\"{luflow_duplicates.shape[0]/luflow2020.shape[0]*100:.2f}% of rows are duplicates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Gv1LGCH-Gm"
      },
      "source": [
        "Step 3. Dataset cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzey4bDbH-pE"
      },
      "outputs": [],
      "source": [
        "# remove rows contain missing value\n",
        "luflow2020 = luflow2020.dropna(how='any')\n",
        "luflow2020.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTE1ik5MIBg-"
      },
      "outputs": [],
      "source": [
        "luflow2020 = luflow2020.drop_duplicates()\n",
        "luflow2020.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXBv9j_IECL"
      },
      "source": [
        "Step 4. Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpJrd3zbIGj4"
      },
      "outputs": [],
      "source": [
        "attack = luflow2020[luflow2020['label']=='malicious']\n",
        "benign = luflow2020[luflow2020['label']=='benign'].sample(n=len(attack)).reset_index(drop=True)\n",
        "\n",
        "luflow2020_exclude_outlier = pd.concat([attack, benign])\n",
        "del attack\n",
        "del benign\n",
        "\n",
        "luflow2020_exclude_outlier['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghj4zH82II9Y"
      },
      "source": [
        "Step 5. Save the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrTme3ixIJey"
      },
      "outputs": [],
      "source": [
        "# function to save the cleaned dataset\n",
        "def save_cleaned_dataset(dataframe: pd.DataFrame,dataset: str, tag: str = \"\"):\n",
        "    # create a new directory to save the cleaned dataset\n",
        "    os.makedirs('./Dataset/dataset_cleaned', exist_ok=True)\n",
        "\n",
        "    if not(tag == \"\"):\n",
        "        tag = \"_\" + tag\n",
        "\n",
        "    dataframe.to_csv(f'Dataset/dataset_cleaned/{dataset}{tag}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSudmZ3pIMMG"
      },
      "outputs": [],
      "source": [
        "save_cleaned_dataset(dataframe=luflow2020_exclude_outlier, dataset='LUFlow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfAVzMPPVRdN"
      },
      "source": [
        "Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQmOO6HOVSGS"
      },
      "outputs": [],
      "source": [
        "# import all necessary packages\n",
        "\n",
        "# basic packages for data processing\n",
        "import os       # to create directories and remove files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random # import random package to specify the random seed\n",
        "\n",
        "# modules for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# modules to interpret the training result\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# silent the warning from the sklearn library\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# set the random seed to ensure the result is reproducible\n",
        "random.seed(10)\n",
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RC8QEwGV4fd"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "luflow = pd.read_csv('Dataset/dataset_cleaned/LUFlow.csv')\n",
        "luflow = luflow.sample(frac=0.05).reset_index(drop=True)\n",
        "luflow.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBmpZc33XTux"
      },
      "outputs": [],
      "source": [
        "luflow['time_between'] = luflow['time_end'] - luflow['time_start']\n",
        "\n",
        "luflow[['time_start', 'time_end', 'time_between', 'duration']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFm9viigh7vB"
      },
      "outputs": [],
      "source": [
        "luflow = luflow.drop(['src_ip', 'dest_ip', 'time_start', 'time_end', 'time_between'], axis=1)\n",
        "luflow.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WVamPhvh_yJ"
      },
      "outputs": [],
      "source": [
        "# seperate the features and the output variables into two dataframe\n",
        "luflow_X = luflow.drop('label', axis=1).copy()\n",
        "luflow_y = luflow['label'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnYpe7r3iNi7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(luflow_X, luflow_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeAi9LpIkxIZ"
      },
      "outputs": [],
      "source": [
        "#extract the importance score\n",
        "score = np.round(rfc.feature_importances_, 3)\n",
        "importance = pd.DataFrame({'feature': luflow_X.columns,\n",
        "                             'importance score': score})\n",
        "importance = importance.sort_values('importance score', ascending=False).set_index('feature')\n",
        "\n",
        "print(f\"Top features: \\n{importance}\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 4)\n",
        "importance.plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-S_iWXClv3v"
      },
      "outputs": [],
      "source": [
        "def save_feature(features: pd.Series, dataset_name: str, algorithm: str, tag: str = \"\"):\n",
        "\n",
        "    feature_count = features.shape[0]\n",
        "\n",
        "    if not(tag == \"\"):\n",
        "        tag = \"_\" + tag\n",
        "\n",
        "    directory = 'Dataset/features_selected'\n",
        "    file_name = f\"{dataset_name}_{algorithm}_{feature_count}{tag}\"\n",
        "\n",
        "    # create directory to save the file\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    features.to_csv(f\"{directory}/{file_name}.csv\", index=False, columns=None)\n",
        "\n",
        "    print(f\"The features have been saved in {directory}/{file_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e5GFVwKmCWz"
      },
      "outputs": [],
      "source": [
        "# save the ranking of the features in a CSV file\n",
        "features = pd.Series(importance.index[:], dtype='str')\n",
        "save_feature(features=features,\n",
        "            dataset_name='LUFlow',\n",
        "            algorithm='RandomForestClassifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PETPSzNLmGl0"
      },
      "outputs": [],
      "source": [
        "# define ML models without optimized hyperparameter\n",
        "models = {\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM6sn7vSmJzq"
      },
      "outputs": [],
      "source": [
        "luflow_train_X, luflow_test_X, luflow_train_y, luflow_test_y = train_test_split(luflow_X, luflow_y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lZmQs_UmNpf"
      },
      "outputs": [],
      "source": [
        "feature_set = []\n",
        "scalar = StandardScaler()\n",
        "scores = []\n",
        "\n",
        "for feature in features:\n",
        "    feature_set.append(feature)\n",
        "    print(f\"Added feature {len(feature_set)} ({feature}) ...\")\n",
        "\n",
        "    test_X = luflow_test_X[feature_set]\n",
        "    train_X = luflow_train_X[feature_set]\n",
        "\n",
        "    # scale the dataset\n",
        "    train_X_scaled = scalar.fit(train_X)\n",
        "    train_X_scaled = scalar.transform(train_X)\n",
        "    test_X_scaled = scalar.transform(test_X)\n",
        "\n",
        "    score_temp = [len(feature_set)]\n",
        "\n",
        "    for model in models:\n",
        "        clf = models[model]\n",
        "        clf.fit(train_X_scaled, luflow_train_y)\n",
        "\n",
        "        prediction = clf.predict(test_X_scaled)\n",
        "        accuracy = metrics.accuracy_score(luflow_test_y, prediction)\n",
        "\n",
        "        score_temp.append(accuracy)\n",
        "\n",
        "    scores.append(score_temp)\n",
        "\n",
        "\n",
        "scores = np.array(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C-kVXrtt4ro"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for index, model in enumerate(models):\n",
        "    plt.plot(scores.T[0], scores.T[index + 1], label=model)\n",
        "\n",
        "plt.rcParams.update({'font.size': 13})\n",
        "plt.title('Accuracy of the models with respect to the number of features')\n",
        "plt.xlabel('Number of features')\n",
        "plt.ylabel('Accuracy score')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9lQ25nXt7sM"
      },
      "outputs": [],
      "source": [
        "# import all of the packages that will be used\n",
        "\n",
        "# basic packages for data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# packages for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# packages to interpret the training result\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# set the random seed to ensure the result is reproducible\n",
        "import random\n",
        "random.seed(10)\n",
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJTpu8xdt-ml"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv('/content/LUFlow.csv').squeeze()\n",
        "features = features[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEJKM9N9uI99"
      },
      "outputs": [],
      "source": [
        "columns = features.tolist() +  ['label']\n",
        "\n",
        "luflow2020 = pd.read_csv('/content/LUFlow_RandomForestClassifier_11.csv', usecols=columns)\n",
        "luflow2020.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B3y1wl5uYpC"
      },
      "outputs": [],
      "source": [
        "luflow2020 = luflow2020.sample(frac=0.05).reset_index(drop=True)\n",
        "print('Class distribution: ')\n",
        "luflow2020['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0564Q8gwvEFP"
      },
      "outputs": [],
      "source": [
        "luflow2020_X = luflow2020.drop('label', axis=1).copy()\n",
        "luflow2020_y = luflow2020['label'].copy()\n",
        "\n",
        "luflow2020_train_X, luflow2020_test_X, luflow2020_train_y, luflow2020_test_y = train_test_split(luflow2020_X, luflow2020_y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "del_TkYBvG3x"
      },
      "outputs": [],
      "source": [
        "scalar = StandardScaler()\n",
        "\n",
        "luflow2020_train_X_scaled = scalar.fit_transform(luflow2020_train_X)\n",
        "luflow2020_test_X_scaled = scalar.transform(luflow2020_test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzeDEdlfvL8b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "parameter_space = [\n",
        "    {'var_smoothing': np.logspace(0, -9, num=100)}\n",
        "]\n",
        "\n",
        "optimal_nb = GridSearchCV(\n",
        "                        GaussianNB(),\n",
        "                        parameter_space,\n",
        "                        cv=5,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=0\n",
        ")\n",
        "\n",
        "optimal_nb.fit(luflow2020_train_X_scaled, luflow2020_train_y)\n",
        "nb_optimal_params = optimal_nb.best_params_\n",
        "print(f\"Optimum hyperparameters: \\n{nb_optimal_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am9_fxnQySKG"
      },
      "outputs": [],
      "source": [
        "prediction = optimal_nb.predict(luflow2020_test_X_scaled)\n",
        "accuracy = metrics.accuracy_score(luflow2020_test_y, prediction)\n",
        "print(f\"Accuracy: {accuracy:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kal_NMP8-FB9"
      },
      "source": [
        "Improvment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FTSO5VHS-Ng2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a519175-e4eb-4ee3-f9e9-2c09f73db083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded: 22,310,482 rows, 7 cols | features: ['dest_port', 'bytes_out', 'total_entropy', 'src_port', 'num_pkts_in', 'duration']\n",
            "n_iterations: 4\n",
            "n_required_iterations: 4\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 17352\n",
            "max_resources_: 468520\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 65\n",
            "n_resources: 17352\n",
            "Fitting 3 folds for each of 65 candidates, totalling 195 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 22\n",
            "n_resources: 52056\n",
            "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 8\n",
            "n_resources: 156168\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 3\n",
            "n_resources: 468504\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "\n",
            "[GaussianNB] best params: {'clf__priors': [0.5, 0.5], 'clf__var_smoothing': np.float64(3.162277660168379e-08)} | cv best F1(mal): 0.8805 | search 172.0s\n",
            "Refit on full train in 89.7s.\n",
            "\n",
            "=== Test Results (GaussianNB only) ===\n",
            "AUC: 0.9600\n",
            "Default 0.5  -> Acc: 0.8398 | F1: 0.8793\n",
            "Best Thr 0.7830 -> Acc: 0.9216 | F1: 0.9315\n",
            "\n",
            "Classification report @ best threshold:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign     0.8708    0.9493    0.9083   2737994\n",
            "   malicious     0.9625    0.9025    0.9315   3955151\n",
            "\n",
            "    accuracy                         0.9216   6693145\n",
            "   macro avg     0.9167    0.9259    0.9199   6693145\n",
            "weighted avg     0.9250    0.9216    0.9220   6693145\n",
            "\n",
            "Confusion matrix @ best threshold:\n",
            " [[2599079  138915]\n",
            " [ 385742 3569409]]\n"
          ]
        }
      ],
      "source": [
        "# ================== GaussianNB (robust preprocessing, fast tuning) ==================\n",
        "import time, numpy as np, pandas as pd, warnings\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import (\n",
        "    f1_score, make_scorer, accuracy_score, roc_auc_score,\n",
        "    precision_recall_curve, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# ---- paths ----\n",
        "FEATURES_PATH = \"/content/LUFlow_RandomForestClassifier_11.csv\"   # your top-k ranking\n",
        "DATASET_PATH  = \"/content/LUFlow.csv\"                              # LUFlow July 2020 cleaned\n",
        "\n",
        "# ---- config ----\n",
        "TOP_K      = 6                  # per paper\n",
        "TEST_SIZE  = 0.30\n",
        "CV_SPLITS  = 3\n",
        "TUNE_FRAC  = 0.03               # 3% subset for very fast tuning on 22M rows (~few hundred K)\n",
        "UPPER_Q    = 0.999              # winsorize cap (per-feature)\n",
        "VERBOSE    = 1\n",
        "\n",
        "# Only NB (no extra models)\n",
        "TUNE_PRIORS = True              # set False to tune only var_smoothing\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Clip each feature at an upper quantile (fast, avoids Yeo-Johnson failures).\"\"\"\n",
        "    def __init__(self, q=0.999):\n",
        "        self.q = q\n",
        "        self.clip_vals_ = None\n",
        "    def fit(self, X, y=None):\n",
        "        X = np.asarray(X, dtype=np.float32)\n",
        "        # nanquantile per column (axis=0)\n",
        "        self.clip_vals_ = np.nanquantile(X, self.q, axis=0)\n",
        "        # guard infinities\n",
        "        self.clip_vals_ = np.where(np.isfinite(self.clip_vals_), self.clip_vals_, np.nanmax(X, axis=0))\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X = np.asarray(X, dtype=np.float32)\n",
        "        # Clip only upper bound; lower bound left as-is (counts/ports are >=0 anyway)\n",
        "        upper = self.clip_vals_.astype(np.float32)\n",
        "        # Broadcast-safe clipping\n",
        "        X = np.minimum(X, upper)\n",
        "        return X\n",
        "\n",
        "def make_log1p():\n",
        "    # feature_names_out='one-to-one' keeps column count consistent\n",
        "    return FunctionTransformer(np.log1p, feature_names_out='one-to-one')\n",
        "\n",
        "# ---------------- Load feature list ----------------\n",
        "feat_df = pd.read_csv(FEATURES_PATH)\n",
        "first_col = feat_df.columns[0]\n",
        "feat_series = feat_df[first_col].astype(str)\n",
        "feat_series = feat_series[~feat_series.str.lower().eq(\"feature\")].reset_index(drop=True)\n",
        "\n",
        "# choose top-k present in dataset\n",
        "dataset_cols = pd.read_csv(DATASET_PATH, nrows=0).columns.tolist()\n",
        "picked_feats = [c for c in feat_series.iloc[:TOP_K].tolist() if c in dataset_cols]\n",
        "missing = [c for c in feat_series.iloc[:TOP_K].tolist() if c not in dataset_cols]\n",
        "if missing:\n",
        "    print(\"⚠️ Skipping missing feature names (not in dataset):\", missing)\n",
        "\n",
        "use_cols = picked_feats + [\"label\"]\n",
        "df = pd.read_csv(DATASET_PATH, usecols=use_cols)\n",
        "\n",
        "# Keep only benign/malicious; coerce to float32\n",
        "df = df[df[\"label\"].isin([\"benign\", \"malicious\"])].copy()\n",
        "for c in picked_feats:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(np.float32)\n",
        "\n",
        "print(f\"Data loaded: {df.shape[0]:,} rows, {df.shape[1]} cols | features: {picked_feats}\")\n",
        "\n",
        "X = df[picked_feats].copy()\n",
        "y = df[\"label\"].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Subsample for tuning to keep search snappy\n",
        "if TUNE_FRAC < 1.0:\n",
        "    rs = np.random.RandomState(RANDOM_STATE)\n",
        "    idx = rs.choice(len(X_train), size=int(len(X_train)*TUNE_FRAC), replace=False)\n",
        "    X_tune, y_tune = X_train.iloc[idx], y_train.iloc[idx]\n",
        "else:\n",
        "    X_tune, y_tune = X_train, y_train\n",
        "\n",
        "# Column groups\n",
        "heavy_cols = [c for c in picked_feats if c in [\"bytes_out\",\"num_pkts_in\",\"duration\",\"src_port\",\"dest_port\"]]\n",
        "light_cols = [c for c in picked_feats if c not in heavy_cols]  # e.g., \"total_entropy\"\n",
        "\n",
        "# Preprocessing:\n",
        "# - heavy: impute -> clip(99.9%) -> log1p -> scale\n",
        "# - light: impute -> scale\n",
        "heavy_pipe = Pipeline([\n",
        "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
        "    (\"clip\",  QuantileClipper(q=UPPER_Q)),\n",
        "    (\"log1p\", make_log1p()),\n",
        "    (\"sc\",    StandardScaler(with_mean=True, with_std=True))\n",
        "])\n",
        "light_pipe = Pipeline([\n",
        "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
        "    (\"sc\",    StandardScaler(with_mean=True, with_std=True))\n",
        "])\n",
        "\n",
        "prep = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"heavy\", heavy_pipe, heavy_cols),\n",
        "        (\"light\", light_pipe, light_cols)\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    n_jobs=None\n",
        ")\n",
        "\n",
        "# Single model: GaussianNB\n",
        "pipe = Pipeline([\n",
        "    (\"prep\", prep),\n",
        "    (\"clf\",  GaussianNB())\n",
        "])\n",
        "\n",
        "# f1 of 'malicious' as scoring (matches paper’s per-class reporting)\n",
        "f1_mal = make_scorer(f1_score, pos_label=\"malicious\")\n",
        "\n",
        "# Parameter grid: var_smoothing (+ optional priors)\n",
        "vs_grid = np.logspace(-12, -6, 13)\n",
        "if TUNE_PRIORS:\n",
        "    prior_grid = [None, [0.50,0.50], [0.45,0.55], [0.40,0.60], [0.35,0.65]]\n",
        "    param_grid = {\"clf__var_smoothing\": vs_grid, \"clf__priors\": prior_grid}\n",
        "else:\n",
        "    param_grid = {\"clf__var_smoothing\": vs_grid}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "search = HalvingGridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    factor=3,\n",
        "    cv=cv,\n",
        "    scoring=f1_mal,\n",
        "    n_jobs=-1,\n",
        "    verbose=VERBOSE,\n",
        "    error_score=np.nan,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "search.fit(X_tune, y_tune)\n",
        "print(f\"\\n[GaussianNB] best params: {search.best_params_} | cv best F1(mal): {search.best_score_:.4f} | search {time.time()-t0:.1f}s\")\n",
        "\n",
        "# Refit on FULL train with the best settings\n",
        "best_gnb = search.best_estimator_\n",
        "t1 = time.time()\n",
        "best_gnb.fit(X_train, y_train)\n",
        "print(f\"Refit on full train in {time.time()-t1:.1f}s.\")\n",
        "\n",
        "# Evaluate on test: default 0.5 + optimized threshold\n",
        "proba = best_gnb.predict_proba(X_test)\n",
        "mal_idx = list(best_gnb.classes_).index(\"malicious\")\n",
        "mal_p  = proba[:, mal_idx]\n",
        "\n",
        "y_true = (y_test.values == \"malicious\").astype(int)\n",
        "\n",
        "# Default 0.5\n",
        "y_pred_05 = (mal_p >= 0.5).astype(int)\n",
        "acc_05 = accuracy_score(y_true, y_pred_05)\n",
        "f1_05  = f1_score(y_true, y_pred_05)\n",
        "auc    = roc_auc_score(y_true, mal_p)\n",
        "\n",
        "# Optimized threshold for F1\n",
        "prec, rec, thr = precision_recall_curve(y_true, mal_p)\n",
        "f1s = (2*prec*rec)/(prec+rec+1e-12)\n",
        "best_idx = np.nanargmax(f1s)\n",
        "best_thr = thr[max(best_idx-1, 0)] if best_idx < len(thr) else 0.5\n",
        "\n",
        "y_pred_opt = (mal_p >= best_thr).astype(int)\n",
        "acc_opt = accuracy_score(y_true, y_pred_opt)\n",
        "f1_opt  = f1_score(y_true, y_pred_opt)\n",
        "\n",
        "print(\"\\n=== Test Results (GaussianNB only) ===\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Default 0.5  -> Acc: {acc_05:.4f} | F1: {f1_05:.4f}\")\n",
        "print(f\"Best Thr {best_thr:.4f} -> Acc: {acc_opt:.4f} | F1: {f1_opt:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report @ best threshold:\")\n",
        "print(classification_report(y_true, y_pred_opt, target_names=[\"benign\",\"malicious\"], digits=4))\n",
        "\n",
        "print(\"Confusion matrix @ best threshold:\\n\", confusion_matrix(y_true, y_pred_opt))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO97wDROD/L9Lr1zZ/zYPLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}